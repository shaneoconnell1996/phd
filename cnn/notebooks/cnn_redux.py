# -*- coding: utf-8 -*-
"""cnn_Redux.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LKnUzpfH1K3vCkm4Qz5ILJBiyXs6XY3B

The old notebook is dead. This is the redux with shiny new functions. Today is August 6th
"""

! rm -rf /usr/local/lib/python3.6/dist-packages/tensorflow*

! pip uninstall tensorflow 
! pip install tensorflow-gpu==2.0.0-alpha

! cp drive/My\ Drive/data/parametric_combat_data.npy ./ 
! cp drive/My\ Drive/data/new_case_labels.txt ./

import tensorflow as tf
tf.test.is_gpu_available()
#tf.config.list_physical_devices('GPU')
print(tf.__version__)

import numpy as np
# this part changes based on what version we run with 
#data = np.load('drive/My Drive/data/cleaned_data.npy')
# data = np.load('drive/My Drive/data/local_and_ukb_dataset_lossy_revised_qc.npy')
# ydata = np.load('drive/My Drive/data/labels.npy')
with open('drive/My Drive/data/new_case_labels.txt') as w:
  ydata=w.readlines()
ydata=[int(i.strip()) for i in ydata]
local_y = ydata[:101]
ukb_y = ydata[101:]

ukb_data = np.load('drive/My Drive/data/ukb_data_cropped.npy')

local_data = np.load('drive/My Drive/data/cropped_local_data.npy')

import pandas as pd 
import numpy as np
import tensorflow as tf
print(f'using tensorflow version {tf.__version__}')
from tensorflow.keras.layers import (
    Dense, Flatten, BatchNormalization, Conv3D,
    SpatialDropout3D, Dropout, Activation, LeakyReLU,
    MaxPooling3D
)
from tensorflow.keras.optimizers import SGD, Adam, RMSprop
from tensorflow.keras.regularizers import l1, l2, l1_l2
from tensorflow.keras.models import Sequential
import matplotlib as mplbck
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,roc_curve,auc,RocCurveDisplay
import sys

local_y=ydata[:101]
local_data=data[:101]
ukb_y=ydata[101:]
ukb_data=data[101:]

# scaling func 
from sklearn.preprocessing import MinMaxScaler, StandardScaler
mn = MinMaxScaler(feature_range=(0,255))
def scale_data_255(patient):
  
  #upper = 255.0
  #scaled_patient = [lower + (upper - lower) * x for x in patient] 
  return mn.fit_transform(patient)

ukb_scale = np.zeros((199,120,140,120))
for i in range(ukb_scale.shape[0]):
  ukb_scale[i] = scale_data_255(ukb_data[i].flatten().reshape(-1,1)).reshape(120,140,120)
#ukbscale=scale_data_255(ukb_data[0].flatten().reshape(-1,1))
#np.max(ukbscale)

local_scale = np.zeros((101,120,140,120))
for i in range(local_scale.shape[0]):
  local_scale[i] = scale_data_255(local_data[i].flatten().reshape(-1,1)).reshape(120,140,120)



import argparse
parser = argparse.ArgumentParser()
parser.add_argument('-c_1','--conv1',dest='conv_1',type=int,required=True,help="The number of units at the first layer of the CNN")
parser.add_argument('-c_2','--conv2',dest='conv_2',type=int,required=True,help="The number of units at the second layer of the CNN")
parser.add_argument('-c_3','--conv3',dest='conv_3',type=int,required=True,help="The number of units at the third layer of the CNN")
parser.add_argument('-c_4','--conv4',dest='conv_4',type=int,required=True,help="The number of units at the fourth layer of the CNN")
parser.add_argument('-c_5','--conv5',dest='conv_5',type=int,required=True,help="The number of units at the fifth layer of the CNN")

parser.add_argument('-a','--activation',dest='activation',type=str,default='relu',
	help="The activation function to be used throughout the CNN. Default is rectified linear unit")
parser.add_argument('-init','--initializer',dest='initializer',type=str,default='uniform',
	help="The weight initialization scheme for the CNN")
parser.add_argument('-c_d','--spatial_dropout',dest='conv_dropout',default=False,action='store_true',
        help="Whether or not to use spatial dropout at the convolutional layers")
parser.add_argument('-c_d_r','--spatial_dropout_rate',dest='cnv_drp_rate',type=float,default=0.1,
        help="The dropout rate for spatial dropout layers")
parser.add_argument('-fs','--filter_size',dest='filt_size',type=int,default=3,
        help="The filter size for the network, which conforms to a 3 * input schema, e.g. 3 will result in a pool size of (3,3,3)")
parser.add_argument('-ps','--pool_size',dest='pool_size',type=int,default=3,
        help="The (max) pooling size for the network, which conforms to a 3 * input schema, e.g. 3 will result in a pool size of (3,3,3)")
parser.add_argument('-l2','--l2_penalty',dest='l2_penalty',type=float,default=0.0,
        help="The L2 penalty regularizer for convolutional layers of the network")
parser.add_argument('-fc_1','--fc_layer_1',dest='ff1',type=int,required=True,
        help="The number of units at the first fully connected layer after flattening")
parser.add_argument('-fc_2','--fc_layer_2',dest='ff2',type=int,required=True,
        help="The number of units at the second fully connected layer after flattening")
parser.add_argument('-fc_3','--fc_layer_3',dest='ff3',type=int,required=True,
        help="The number of units at the third fully connected layer after flattening")
parser.add_argument('-fc_4','--fc_layer_4',dest='ff4',type=int,required=True,
        help="The number of units at the fourth fully connected layer after flattening")
parser.add_argument('-bn','--batch_norm',dest='batch_norm',action='store_true',
        help="Whether or not to use batch normalization throughout the fully connected layers")
parser.add_argument('-bn_m','--batch_norm_momentum',dest='mom',type=float,default=0.9,
        help="The momentum hyperparameter for batch normalisation layers")
parser.add_argument('-fc_d','--fc_dropout',dest='ffdrop',required=True,action='store_true',
        help="Whether or not to add dropout at the fully connected layers")
parser.add_argument('-fc_d_r','--fc_dropout_rate',dest='drp_rate',type=float,default=0.2,
        help="The rate at which to apply dropout in the fully connected layers")
parser.add_argument('-p','--padding',dest='padding',type=str,required=True,default='valid',
        help="Padding schema to use, where options are 'valid' and 'same' ")

# next set of args are fed to compiler function 
parser.add_argument('-opt','--optimizer',dest='optimizer',type=str,required=True,
        help="Optimizer to use: choice of rmsprop, adam, or stochastic gradient descent")
parser.add_argument('-eps','--epsilon',dest='epsilon',type=float,default=1e-7,
        help="Epsilon constant used during update steps (for ADAM, the default may not be optimal, and may suit values of 0.1 or 1.0 instead)")
parser.add_argument('-m','--momentum',dest='momentum',type=float,default=0.95,
        help="Momentum term to use for either RMSprop or SGD (generally values closer to 1 are preferred)")
parser.add_argument('-nm','--nesterov',dest='nesterov',action='store_true',default=True,
        help="Whether or not to use nesterov momentum with SGD (slightly updates usual momentum term to reflect intermediate parameter value)")
parser.add_argument('-lr','--learning-rate',dest='lr',type=float,default=0.01,
	help="The learning rate of the CNN")
# parse arguments
#parser.add_argument('-data','--datapath',dest='datapath',type=str,required=True,
 #       help="The data to use for training")
#parser.add_argument('-y','--ypath',dest='ypath',type=str,required=True,
#        help="The labels to use for training")
parser.add_argument('-seed','--seed',dest='seed',type=int,default=13,
        help="Random seed to use for training")
parser.add_argument('-bs','--batch_size',dest='batch_size',type=int,default=5,
        help="Batch size to use during training")
parser.add_argument('-ts','--test_size',dest='test_size',type=float,default=0.3,
        help="Train test split size to use")
parser.add_argument('-vs','--val_size',dest='val_size',type=float,default=0.3,
        help="Train test split size to use")
parser.add_argument('-v','--verbosity',dest='verbosity',type=int,default=0,
        help="Verbosity during training")
parser.add_argument('-ep','--epochs',dest='epochs',type=int,default=10,
        help="How long to train")
def create_summary(args):
    df = pd.DataFrame.from_dict(vars(args),orient='index')
    df['fields']=df.index
    df.columns = ['values','fields']
    return(df)

def create_model(conv_1,conv_2,conv_3,conv_4,conv_5,activation,initializer,conv_dropout,cnv_drp_rate,
                filt_size,pool_size,l2_penalty,ff1,ff2,ff3,ff4,batch_norm,mom,ffdrop,drp_rate,
                padding,datashape):
    
    """creates convolutional neural network with tensorflow keras layers in 3 distinct blocks"""
    model=Sequential()
    
    model.add(Conv3D(conv_1,filt_size,input_shape=datashape,activation=activation,
                    kernel_initializer=initializer,
                    name='conv_1',padding=padding))
    model.add(Conv3D(conv_2,filt_size,activation=activation,
                    kernel_initializer=initializer,
                    name='conv_2',padding=padding))
    if conv_dropout == True:
        model.add(SpatialDropout3D(cnv_drp_rate,name='spatial_dropout_1'))
    model.add(MaxPooling3D(pool_size,name='max_pool_1'))
    
    
    model.add(Conv3D(conv_3,filt_size,activation=activation,
                    kernel_initializer=initializer,
                    name='conv_3',padding=padding))
    model.add(Conv3D(conv_4,filt_size,activation=activation,
                    kernel_initializer=initializer,
                    name='conv_4',padding=padding))
    if conv_dropout == True:
        model.add(SpatialDropout3D(cnv_drp_rate,name='spatial_dropout_2'))
    model.add(MaxPooling3D(pool_size,name='max_pool_2'))
    
    
    model.add(Conv3D(conv_5,filt_size,activation=activation,
                    kernel_initializer=initializer,
                    name='conv_5',padding=padding))
    if conv_dropout == True:
        model.add(SpatialDropout3D(cnv_drp_rate,name='spatial_dropout_3'))
    model.add(MaxPooling3D(pool_size,name='max_pool_3'))
    
    model.add(Flatten(name='flatten'))
    
    
    # checkpoint
    
    model.add(Dense(ff1,activation=activation,
                    kernel_initializer=initializer,
                   name='dense_1'))
    
    if batch_norm==True:
        model.add(BatchNormalization(momentum=mom,name='batch_norm_1'))
    if ffdrop==True:
        model.add(Dropout(drp_rate,name='dense_dropout_1'))
    model.add(Dense(ff2,activation=activation,
                    kernel_initializer=initializer,
                   name='dense_2'))
    if batch_norm==True:
        model.add(BatchNormalization(momentum=mom,name='batch_norm_2'))
    if ffdrop==True:
        model.add(Dropout(drp_rate,name='dense_dropout_2'))
    model.add(Dense(ff3,activation=activation,
                    kernel_initializer=initializer,
                   name='dense_3'))
    if batch_norm==True:
        model.add(BatchNormalization(momentum=mom,name='batch_norm_3'))
    if ffdrop==True:
        model.add(Dropout(drp_rate,name='dense_dropout_3'))
    model.add(Dense(ff4,activation=activation,
                    kernel_initializer=initializer,
                   name='dense_4'))
    
    model.add(Dense(1,activation='sigmoid',name='prediction'))
    
    return(model)

def compile_nn(model,optimizer,lr,epsilon,momentum,nesterov):
    """compiles given model with specified params"""
    if optimizer=='sgd':
        if nesterov == True:
            model.compile(optimizer=SGD(learning_rate=lr,momentum=momentum,nesterov=True),metrics=['accuracy'],
                         loss='binary_crossentropy')
        else:
            model.compile(optimizer=SGD(learning_rate=lr,momentum=momentum,nesterov=False),metrics=['accuracy'],
                         loss='binary_crossentropy')
    if optimizer=='adam':
        model.compile(optimizer=Adam(learning_rate=lr,epsilon=epsilon),metrics=['accuracy'],
                         loss='binary_crossentropy')
    if optimizer=='rmsprop':
        model.compile(optimizer=RMSprop(learning_rate=lr,momentum=momentum,epsilon=epsilon),metrics=['accuracy'],
                         loss='binary_crossentropy')
        

##### create training function that takes epochs to run as another parameter 


def train_model(epochs,verbose,valsize,nn,x,y,bs):
    # Trains the given nn according to input params and returns the progress object
    progress=nn.fit(x,y,epochs=epochs,verbose=verbose,validation_split=valsize,batch_size=bs,shuffle=True)
    return(progress)

def plot_and_save_prog(progress,val):
    # plots and saves model
    fig,(ax1,ax2) = plt.subplots(ncols=2,nrows=1,figsize=(15,8))
    ax1.plot(progress.history['accuracy'],label='Training')
    if val == True:
      ax1.plot(progress.history['val_accuracy'],label='Validation')
    ax1.set_title('Accuracy vs. Epochs')
    ax1.set_ylabel('Accuracy')
    ax1.set_xlabel('Epoch')
    ax1.legend()

    ax2.plot(progress.history['loss'],label='Training')
    if val == True:

      ax2.plot(progress.history['val_loss'],label='Validation')
    ax2.set_title('Loss vs. Epochs')
    ax2.set_ylabel('Loss')
    ax2.set_xlabel('Epoch')
    ax2.legend()
    
    #fig.savefig(f'models/{mname}/learning_curve.png')

def eval_performance(model,test,true):
    pred_prob=[]
    for i in range(test.shape[0]):
      pred_prob.append(float(model.predict(np.array([test[i]]))))
    y_pred=[]
    for i in pred_prob:
      if i > 0.5:
        y_pred.append(1)
      else:
        y_pred.append(0)
    return(accuracy_score(true,y_pred),y_pred,pred_prob)

def make_roc_curve(y_prob,acc,truey,plot=True):
    fpr,tpr,_=roc_curve(truey,y_prob)
    auc_cnn=auc(fpr,tpr)
    if plot==True:
      fig,ax=plt.subplots(figsize=(15,8))
      ax.plot(fpr,tpr,label=f'CNN acc = {acc}')
      ax.plot([0,1],c='red',label='Baseline')
      ax.legend()
      ax.text(x=0.8,y=0.05,s=f'AUC={auc_cnn}')
      ax.set_xlabel('False positive rate')
      ax.set_ylabel('True positive rate')
      ax.set_title('ROC curve with AUC')
    else:
      return auc_cnn

# train test split and reshape # change as needed
# data=data.reshape(data.shape[0],125,145,125)
#s = data.shape
#s = s+(1,)
#data=data.reshape(s)
#data.shape
#x_train,x_test,y_train,y_test=train_test_split(data,ydata,random_state=13,
                                        #       test_size=0.2)

s = ukb_data.shape
s = s+(1,)
ukb_data=ukb_data.reshape(s)
s = local_data.shape
s = s+(1,)
local_data=local_data.reshape(s)

ukb_data.shape

args=parser.parse_args(['-c_1', '16', '-c_2', '16',
                        '-c_3', '32', '-c_4', '32', '-c_5', '64', '-a', 'relu',
                        '-init', 'glorot_uniform', '-c_d', '-c_d_r', '0.1', '-fs', '3',
                        '-ps', '3','-l2', '0.01', '-fc_1', '64', '-fc_2', '64', '-fc_3', '32', '-fc_4', '32',
                        '-bn', '-bn_m', '0.9', '-fc_d', '-fc_d_r', '0.1', '-p', 'valid', 
                        '-opt', 'rmsprop', '-eps', '1e-7', '-m', '0.9', '-lr', '0.005', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-vs', '0.2','-v', '1', '-ep', '150'])
# create model summary

create_summary(args=args)
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)

# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)

args=parser.parse_args(['-c_1', '16', '-c_2', '16',
                        '-c_3', '32', '-c_4', '32', '-c_5', '64', '-a', 'relu',
                        '-init', 'he_normal', '-c_d', '-c_d_r', '0.1', '-fs', '3',
                        '-ps', '3','-l2', '0.1', '-fc_1', '64', '-fc_2', '64', '-fc_3', '32', '-fc_4', '32',
                        '-bn', '-bn_m', '0.9', '-fc_d', '-fc_d_r', '0.1', '-p', 'valid', 
                        '-opt', 'adam', '-eps', '1e-7', '-m', '0.9', '-lr', '0.003', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-vs', '0.2','-v', '1', '-ep', '150'])
# create model summary

create_summary(args=args)
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)

# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)

args=parser.parse_args(['-c_1', '16', '-c_2', '16',
                        '-c_3', '32', '-c_4', '32', '-c_5', '64', '-a', 'relu',
                        '-init', 'he_normal', '-c_d', '-c_d_r', '0.1', '-fs', '3',
                        '-ps', '3','-l2', '0.1', '-fc_1', '100', '-fc_2', '70', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.05', '-p', 'valid', 
                        '-opt', 'sgd', '-eps', '1e-7', '-m', '0.9', '-lr', '0.003', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-vs', '0.2','-v', '1', '-ep', '150','-nm'])
# create model summary

create_summary(args=args)
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)

# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)

args=parser.parse_args(['-c_1', '16', '-c_2', '16',
                        '-c_3', '32', '-c_4', '32', '-c_5', '64', '-a', 'relu',
                        '-init', 'he_normal', '-c_d_r', '0.1', '-fs', '3',
                        '-ps', '3','-l2', '0.1', '-fc_1', '100', '-fc_2', '70', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.05', '-p', 'valid', 
                        '-opt', 'adam', '-eps', '0.01', '-m', '0.9', '-lr', '0.003', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-vs', '0.2','-v', '1', '-ep', '150','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)

# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)

acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

args=parser.parse_args(['-c_1', '32', '-c_2', '32',
                        '-c_3', '64', '-c_4', '64', '-c_5', '128', '-a', 'relu',
                        '-init', 'he_normal', '-c_d_r', '0.1', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '100', '-fc_2', '70', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.2', '-p', 'same', 
                        '-opt', 'adam', '-eps', '0.01', '-m', '0.9', '-lr', '0.0005', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-bn','-bn_m','0.9',
                        '-vs', '0.25','-v', '1', '-ep', '150','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)
acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

args=parser.parse_args(['-c_1', '32', '-c_2', '32',
                        '-c_3', '64', '-c_4', '64', '-c_5', '128', '-a', 'relu',
                        '-init', 'he_normal', '-c_d_r', '0.1', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '100', '-fc_2', '70', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.2', '-p', 'same', 
                        '-opt', 'adam', '-eps', '0.01', '-m', '0.95', '-lr', '0.0005', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-bn','-bn_m','0.9',
                        '-vs', '0.25','-v', '1', '-ep', '150','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)
acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

args=parser.parse_args(['-c_1', '32', '-c_2', '32',
                        '-c_3', '64', '-c_4', '64', '-c_5', '128', '-a', 'relu',
                        '-init', 'he_normal', '-c_d_r', '0.1', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '100', '-fc_2', '70', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.2', '-p', 'same', 
                        '-opt', 'adam', '-eps', '1e-7', '-m', '0.9', '-lr', '0.0005', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-bn','-bn_m','0.9',
                        '-vs', '0.25','-v', '1', '-ep', '150','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)
acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

args=parser.parse_args(['-c_1', '32', '-c_2', '32',
                        '-c_3', '64', '-c_4', '64', '-c_5', '128', '-a', 'relu',
                        '-init', 'he_normal', '-c_d',
                        '-c_d_r', '0.1', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '100', '-fc_2', '70', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.2', '-p', 'same', 
                        '-opt', 'adam', '-eps', '0.01', '-m', '0.9', '-lr', '0.0005', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-bn','-bn_m','0.9',
                        '-vs', '0.25','-v', '1', '-ep', '150','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)
acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

args=parser.parse_args(['-c_1', '32', '-c_2', '32',
                        '-c_3', '64', '-c_4', '64', '-c_5', '128', '-a', 'relu',
                        '-init', 'he_normal', '-c_d',
                        '-c_d_r', '0.15', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '100', '-fc_2', '70', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.2', '-p', 'same', 
                        '-opt', 'adam', '-eps', '0.01', '-m', '0.9', '-lr', '0.0005', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-bn','-bn_m','0.9',
                        '-vs', '0.25','-v', '1', '-ep', '200','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)
acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

args=parser.parse_args(['-c_1', '32', '-c_2', '32',
                        '-c_3', '64', '-c_4', '64', '-c_5', '128', '-a', 'relu',
                        '-init', 'he_normal', '-c_d',
                        '-c_d_r', '0.05', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '100', '-fc_2', '70', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.2', '-p', 'same', 
                        '-opt', 'adam', '-eps', '0.01', '-m', '0.9', '-lr', '0.0003', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-bn','-bn_m','0.9',
                        '-vs', '0.25','-v', '1', '-ep', '200','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)
acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

args=parser.parse_args(['-c_1', '32', '-c_2', '32',
                        '-c_3', '64', '-c_4', '64', '-c_5', '128', '-a', 'relu',
                        '-init', 'glorot_uniform',
                        '-c_d_r', '0.05', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '100', '-fc_2', '100', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.2', '-p', 'valid', 
                        '-opt', 'rmsprop', '-eps', '1e-7', '-m', '0.93', '-lr', '0.0003', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-bn','-bn_m','0.9',
                        '-vs', '0.25','-v', '1', '-ep', '200','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)
acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

args=parser.parse_args(['-c_1', '32', '-c_2', '32',
                        '-c_3', '64', '-c_4', '64', '-c_5', '128', '-a', 'relu',
                        '-init', 'glorot_uniform',
                        '-c_d_r', '0.05', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '100', '-fc_2', '100', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.2', '-p', 'valid', 
                        '-opt', 'rmsprop', '-eps', '1e-7', '-m', '0.9', '-lr', '0.0003', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-bn','-bn_m','0.95',
                        '-vs', '0.25','-v', '1', '-ep', '200','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)
acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

args=parser.parse_args(['-c_1', '32', '-c_2', '32',
                        '-c_3', '64', '-c_4', '64', '-c_5', '128', '-a', 'relu',
                        '-init', 'glorot_uniform',
                        '-c_d_r', '0.05', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '100', '-fc_2', '100', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.2', '-p', 'valid', 
                        '-opt', 'rmsprop', '-eps', '1e-7', '-m', '0.93', '-lr', '0.0003', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-bn','-bn_m','0.9',
                        '-vs', '0.25','-v', '1', '-ep', '50','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)
acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

args=parser.parse_args(['-c_1', '32', '-c_2', '32',
                        '-c_3', '64', '-c_4', '64', '-c_5', '128', '-a', 'relu',
                        '-init', 'glorot_uniform',
                        '-c_d_r', '0.05', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '100', '-fc_2', '100', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.3', '-p', 'valid', 
                        '-opt', 'rmsprop', '-eps', '1e-7', '-m', '0.93', '-lr', '0.0003', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-bn_m','0.9',
                        '-vs', '0.25','-v', '1', '-ep', '200','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)
acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

args=parser.parse_args(['-c_1', '32', '-c_2', '32',
                        '-c_3', '64', '-c_4', '64', '-c_5', '128', '-a', 'relu',
                        '-init', 'glorot_uniform',
                        '-c_d_r', '0.05', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '100', '-fc_2', '100', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.3', '-p', 'valid', 
                        '-opt', 'rmsprop', '-eps', '1e-7', '-m', '0.93', '-lr', '0.0003', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-bn','-bn_m','0.87',
                        '-vs', '0.25','-v', '1', '-ep', '150','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)
acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

args=parser.parse_args(['-c_1', '32', '-c_2', '32',
                        '-c_3', '64', '-c_4', '64', '-c_5', '128', '-a', 'relu',
                        '-init', 'glorot_uniform',
                        '-c_d_r', '0.05', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '100', '-fc_2', '100', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.3', '-p', 'same', 
                        '-opt', 'rmsprop', '-eps', '1e-7', '-m', '0.93', '-lr', '0.0003', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-bn','-bn_m','0.83',
                        '-vs', '0.25','-v', '1', '-ep', '150','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)
acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

args=parser.parse_args(['-c_1', '32', '-c_2', '32',
                        '-c_3', '64', '-c_4', '64', '-c_5', '128', '-a', 'relu',
                        '-init', 'glorot_uniform',
                        '-c_d_r', '0.05', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '100', '-fc_2', '100', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.35', '-p', 'same', 
                        '-opt', 'rmsprop', '-eps', '1e-7', '-m', '0.93', '-lr', '0.0003', '-seed', '13', '-bs', '6', '-ts', '0.2',
                        '-bn','-bn_m','0.8',
                        '-vs', '0.25','-v', '1', '-ep', '50','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)
acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

args=parser.parse_args(['-c_1', '32', '-c_2', '32',
                        '-c_3', '64', '-c_4', '64', '-c_5', '64', '-a', 'relu',
                        '-init', 'glorot_uniform',
                        '-c_d_r', '0.1', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '100', '-fc_2', '100', '-fc_3', '50', '-fc_4', '30',
                        '-fc_d', '-fc_d_r', '0.35', '-p', 'same', 
                        '-opt', 'adam', '-eps', '1e-7', '-m', '0.93', '-lr', '0.006', '-seed', '13', '-bs', '6', '-ts', '0.3',
                        '-vs', '0.1','-v', '1', '-ep', '30','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=x_train,y=y_train,bs=args.batch_size)
plot_and_save_prog(progress=progress)
acc,_,yprob = eval_performance(nn)
make_roc_curve(y_prob=yprob,acc=acc)

"""August 17th - try sparser models with 10 fold cross validation."""

from sklearn.model_selection import StratifiedKFold
def create_cnn_kfold(lr):
  cnn = Sequential([
                  Conv3D(32,(3,3,3),data_format='channels_last',input_shape=(125,145,125,1),
                         activation='relu'),
                   Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((3,3,3)),
                   Conv3D(32,(3,3,3),activation='relu'),
                   Conv3D(32,(3,3,3),activation='relu'),
                   MaxPooling3D((3,3,3)),
                  Conv3D(32,(3,3,3),activation='relu'),
                   Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((3,3,3)),

                   Flatten(),

                   Dense(250,activation='relu'),
                   Dropout(0.25),
                   Dense(100,activation='relu'),
                   Dropout(0.25),
                   Dense(100,activation='relu'),
                   Dropout(0.25),
                   Dense(50,activation='relu'),
                   Dense(1,activation='sigmoid')
  ])
  cnn.compile(optimizer=Adam(learning_rate=lr),metrics=['accuracy'],loss='binary_crossentropy')
  return cnn
cnn = create_cnn_kfold(0.009)

def cross_val_cnn(folds,x,y,niter,bs,vs):
    
    skf = StratifiedKFold(n_splits=folds,shuffle=True,random_state=42)
    accs = []
    bp = y
    bp = np.array(bp)
    skf.get_n_splits(x,bp)
    for train_index,test_index in skf.split(x,bp):
        nn = create_cnn_kfold(0.009)
        nn.fit(x[train_index],bp[train_index],epochs=niter,verbose=1,batch_size=bs,
               validation_split=vs)
        preds = [nn.predict(np.array([x[i]])) for i in test_index]
        preds = [1 if i > 0.5 else 0 for i in preds]
        acc = accuracy_score(bp[test_index],preds,normalize=True)

        accs.append(acc)
    return(f'{folds} fold cross val (conv neural net): {np.mean(accs)} +/- {np.std(accs)}')

cross_val_cnn(folds=5,x=data,y=ydata,niter=50,bs=5,vs=0)

def create_cnn_sparse(lr):
  cnn = Sequential([
                  Conv3D(32,(3,3,3),data_format='channels_last',input_shape=(125,145,125,1),
                         activation='relu'),
                   Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((3,3,3)),
                   Conv3D(32,(3,3,3),activation='relu'),
                   Conv3D(32,(3,3,3),activation='relu'),
                   MaxPooling3D((3,3,3)),
                  Conv3D(32,(3,3,3),activation='relu'),
                    #Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((3,3,3)),

                   Flatten(),

                   Dense(512,activation='relu'),
                   Dropout(0.25),
                   Dense(256,activation='relu'),
                   Dropout(0.25),
                   Dense(128,activation='relu'),
                   Dropout(0.25),
                   Dense(64,activation='relu'),
                   Dense(1,activation='sigmoid')
  ])
  cnn.compile(optimizer=Adam(learning_rate=lr),metrics=['accuracy'],loss='binary_crossentropy')
  return cnn
cnn = create_cnn_sparse(0.006)
cnn.summary()

# training 
cnn.fit(x_train,y_train,epochs=30,shuffle=True,verbose=1,batch_size=5)

cnn.fit(x_train,y_train,epochs=100,shuffle=True,verbose=1,batch_size=5)

args=parser.parse_args(['-c_1', '16', '-c_2', '16',
                        '-c_3', '32', '-c_4', '32', '-c_5', '64', '-a', 'relu',
                        '-init', 'glorot_uniform', '-c_d_r', '0.1', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '264', '-fc_2', '128', '-fc_3', '64', '-fc_4', '32',
                        '-fc_d', '-fc_d_r', '0.2', '-p', 'valid', 
                        '-opt', 'sgd', '-eps', '1e-7', '-m', '0.9', '-lr', '0.003', '-seed', '13', '-bs', '3', '-ts', '0.2',
                        '-bn','-bn_m','0.9',
                        '-vs', '0.0','-v', '1', '-ep', '50','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding,
                  datashape=ukb_data[0].shape)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=ukb_data,y=ukb_y,bs=args.batch_size)
plot_and_save_prog(progress=progress,val=False)
acc,_,yprob = eval_performance(nn,local_data,local_y)
make_roc_curve(y_prob=yprob,acc=acc,truey=local_y)

acc,_,yprob = eval_performance(nn,local_data,local_y)
make_roc_curve(y_prob=yprob,acc=acc,truey=local_y)

args=parser.parse_args(['-c_1', '16', '-c_2', '16',
                        '-c_3', '32', '-c_4', '32', '-c_5', '64', '-a', 'relu',
                        '-init', 'glorot_uniform', '-c_d_r', '0.1', '-fs', '3',
                        '-ps', '3','-l2', '0', '-fc_1', '264', '-fc_2', '128', '-fc_3', '64', '-fc_4', '32',
                        '-fc_d', '-fc_d_r', '0.2', '-p', 'valid', 
                        '-opt', 'adam', '-eps', '1e-7', '-m', '0.9', '-lr', '0.0009', '-seed', '13', '-bs', '4', '-ts', '0.2',
                        '-bn','-bn_m','0.85',
                        '-vs', '0.0','-v', '1', '-ep', '50','-nm'])
# create model summary

print(create_summary(args=args))
# create model
filt_size=(args.filt_size,args.filt_size,args.filt_size)
pool_size=(args.pool_size,args.pool_size,args.pool_size)
nn = create_model(conv_1=args.conv_1,conv_2=args.conv_2,conv_3=args.conv_3,conv_4=args.conv_4,conv_5=args.conv_5,
                  activation=args.activation,initializer=args.initializer,conv_dropout=args.conv_dropout,cnv_drp_rate=args.cnv_drp_rate,
                 filt_size=filt_size,pool_size=pool_size,l2_penalty=args.l2_penalty,ff1=args.ff1,ff2=args.ff2,ff3=args.ff3,ff4=args.ff4,
                 batch_norm=args.batch_norm,mom=args.mom,ffdrop=args.ffdrop,drp_rate=args.drp_rate,padding=args.padding,
                  datashape=ukb_data[0].shape)
print(nn.summary())
# compile and train model 

compile_nn(model=nn,optimizer=args.optimizer,lr=args.lr,epsilon=args.epsilon,momentum=args.momentum,nesterov=args.nesterov)
progress = train_model(epochs=args.epochs,verbose=args.verbosity,valsize=args.val_size,nn=nn,x=ukb_data,y=ukb_y,bs=args.batch_size)
plot_and_save_prog(progress=progress,val=False)
acc,_,yprob = eval_performance(nn,local_data,local_y)
make_roc_curve(y_prob=yprob,acc=acc,truey=local_y)

import tensorflow as tf 
tf.__version__



def create_cnn_sparse(lr,shape):
  cnn = Sequential([
                  Conv3D(32,(3,3,3),data_format='channels_last',input_shape=shape,
                         activation='relu'),
                   Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((3,3,3)),
                   Conv3D(64,(3,3,3),activation='relu'),
                   Conv3D(64,(3,3,3),activation='relu'),
                   MaxPooling3D((3,3,3)),
                  Conv3D(128,(3,3,3),activation='relu'),
                    #Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((2,2,2)),

                   Flatten(),

                   Dense(512,activation='relu'),
                   Dropout(0.1),
                   Dense(256,activation='relu'),
                   Dropout(0.1),
                   Dense(128,activation='relu'),
                   Dropout(0.1),
                   Dense(64,activation='relu'),
                   Dense(1,activation='sigmoid')
  ])
  cnn.compile(optimizer=Adam(learning_rate=lr,clipnorm=1),metrics=['accuracy'],loss='binary_crossentropy')
  return cnn
cnn = create_cnn_sparse(0.0009,local_data.reshape(101,120,140,120,1)[0].shape)
cnn.summary()
pr = cnn.fit(x=ukb_scale.reshape(199,120,140,120,1),y=ukb_y,epochs=50,shuffle=True,batch_size=6)
plot_and_save_prog(pr,val=False)
acc,_,yprob = eval_performance(cnn,local_scale.reshape(101,120,140,120,1),local_y)
make_roc_curve(y_prob=yprob,acc=acc,truey=local_y)

def create_cnn_sparse(lr,shape):
  cnn = Sequential([
                  Conv3D(32,(3,3,3),data_format='channels_last',input_shape=shape,
                         activation='relu'),
                   Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((3,3,3)),
                   Conv3D(64,(3,3,3),activation='relu'),
                   Conv3D(64,(3,3,3),activation='relu'),
                   MaxPooling3D((3,3,3)),
                  Conv3D(128,(3,3,3),activation='relu'),
                    #Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((2,2,2)),

                   Flatten(),

                   Dense(512,activation='relu'),
                   Dropout(0.1),
                   Dense(256,activation='relu'),
                   Dropout(0.1),
                   Dense(128,activation='relu'),
                   Dropout(0.1),
                   Dense(64,activation='relu'),
                   Dense(1,activation='sigmoid')
  ])
  cnn.compile(optimizer=Adam(learning_rate=lr,clipnorm=1),metrics=['accuracy'],loss='binary_crossentropy')
  return cnn
cnn = create_cnn_sparse(0.0007,local_data.reshape(101,120,140,120,1)[0].shape)
cnn.summary()
pr = cnn.fit(x=ukb_scale.reshape(199,120,140,120,1),y=ukb_y,epochs=100,shuffle=True,batch_size=6)
plot_and_save_prog(pr,val=False)
acc,_,yprob = eval_performance(cnn,local_scale.reshape(101,120,140,120,1),local_y)
make_roc_curve(y_prob=yprob,acc=acc,truey=local_y)

cnn.save('drive/My Drive/data/auc_062_acc_61.h5')

"""Bookmark 1 

this is the best model to date. Go from this later on tomorrow!

I would like to create a custom training loop that starts taking prediction metrics after a certain point - it will be one epoch of fit with shuffle on, then prediction, then printing the resulting metrics that we are tracking to the screen before finally plotting everything at the end
"""

# aug27custom_loop model arch 
def create_cnn_sparse(lr,shape):
  cnn = Sequential([
                  Conv3D(32,(3,3,3),data_format='channels_last',input_shape=shape,
                         activation='relu'),
                   Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((3,3,3)),
                   Conv3D(64,(3,3,3),activation='relu'),
                   Conv3D(64,(3,3,3),activation='relu'),
                   MaxPooling3D((3,3,3)),
                  Conv3D(128,(3,3,3),activation='relu'),
                    #Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((2,2,2)),

                   Flatten(),

                   Dense(512,activation='relu'),
                   Dropout(0.1),
                   Dense(256,activation='relu'),
                   Dropout(0.1),
                   Dense(128,activation='relu'),
                   Dropout(0.1),
                   Dense(64,activation='relu'),
                   Dense(1,activation='sigmoid')
  ])
  cnn.compile(optimizer=Adam(learning_rate=lr,clipnorm=1),metrics=['accuracy'],loss='binary_crossentropy')
  return cnn

# custom training loop 
cnn = create_cnn_sparse(0.0007,local_data.reshape(101,120,140,120,1)[0].shape)

def custom_loop(model,epochs,burnin,plot,thresh):
  acc = []
  loss=[]
  aucs = []
  accus = []
  for i in range(epochs):
    print(f'epoch {i+1} fit:')
    pr = model.fit(x=ukb_scale.reshape(199,120,140,120,1),y=ukb_y,epochs=1,shuffle=True,batch_size=6,
                   verbose=1)
    acc.append(pr.history['accuracy'])
    loss.append(pr.history['loss'])
    #plot_and_save_prog(pr,val=False)
    if i > burnin:
      accu,_,yprob = eval_performance(model,local_scale.reshape(101,120,140,120,1),local_y)
      aucs.append(make_roc_curve(y_prob=yprob,acc=accu,truey=local_y,plot=False))
      accus.append(accu)
      if aucs[-1] > thresh:
        model.save(f'aug27custom_loop_auc_60_epoch{i}.h5')  
  if plot==True:
    fig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(15,8))
    ax1.plot(acc,c='blue')
    ax1.set_title('Training accuracy')
    ax2.plot(loss,c='red')
    ax2.set_title('Training loss')

    accu,_,yprob = eval_performance(model,local_scale.reshape(101,120,140,120,1),local_y)
    aucs.append(make_roc_curve(y_prob=yprob,acc=accu,truey=local_y,plot=True))
    

    fig,ax = plt.subplots(figsize=(15,8))
    ax.plot(aucs,c='green',label='AUCs')
    #ax.set_xticks(np.arange(burnin,epochs))
    ax.set_xticklabels(np.arange(burnin+1,epochs+1))
    ax.set_title('AUCs vs epochs after burn in')
    ax.plot(accus,c='red',label='Test Accuracies')
    ax.legend()

    
custom_loop(cnn,100,55,True,0.6)

# need clause to prevent fitting for too long before burnin

def custom_loop(model,epochs,burnin,plot,thresh,acc_thresh,mdlnames,npredictions):
  acc = []
  loss=[]
  aucs = []
  accus = []
  for i in range(epochs):
    print(f'epoch {i+1} fit:')
    pr = model.fit(x=ukb_scale.reshape(199,120,140,120,1),y=ukb_y,epochs=1,shuffle=True,batch_size=6,
                   verbose=1)
    acc.append(pr.history['accuracy'])
    loss.append(pr.history['loss'])
    acc_val = acc[-1][0]
    if acc_val > acc_thresh and i < burnin:
      print(f'Accuracy above threshold before burn in. New burn in assigned at {i}')
      burnin = i
    else:
      pass
    if i >= burnin:
      accu,_,yprob = eval_performance(model,local_scale.reshape(101,120,140,120,1),local_y)
      aucs.append(make_roc_curve(y_prob=yprob,acc=accu,truey=local_y,plot=False))
      accus.append(accu)
      if aucs[-1] > thresh:
        model.save(f'{mdlnames}_epoch{i}.h5')  
    if i - burnin > npredictions:
      print(f'Model has been storing predictions for {npredictions} epochs. Terminating loop')
      break
    
  if plot==True:
    fig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(15,8))
    ax1.plot(acc,c='blue')
    ax1.set_title('Training accuracy')
    ax2.plot(loss,c='red')
    ax2.set_title('Training loss')

    accu,_,yprob = eval_performance(model,local_scale.reshape(101,120,140,120,1),local_y)
    make_roc_curve(y_prob=yprob,acc=accu,truey=local_y,plot=True)
    

    fig,ax = plt.subplots(figsize=(15,8))
    ax.plot(aucs,c='green',label='AUCs')
    ax.set_title('AUCs vs epochs after burn in')
    ax.plot(accus,c='red',label='Test Accuracies')
    ax.legend()

cnn = create_cnn_sparse(0.0007,local_data.reshape(101,120,140,120,1)[0].shape)
#custom_loop(cnn,120,70,True,0.6,0.7)

import glob 
filelist = glob.glob('/content/*epoch1*.h5')
filelist

# eval prospective model
import glob 
filelist = glob.glob('/content/*epoch1*.h5')
filelist
for m in filelist:
  print(m)
  md = tf.keras.models.load_model(m)

  accu,_,yprob = eval_performance(md,local_scale.reshape(101,120,140,120,1),local_y)
  make_roc_curve(y_prob=yprob,acc=accu,truey=local_y,plot=True)
  plt.show()

"""Bookmark 2 
These models are performing pretty well! I could try to up the dropout rate.
"""

def create_cnn_sparse_drop(lr,shape,drprate):
  cnn = Sequential([
                  Conv3D(32,(3,3,3),data_format='channels_last',input_shape=shape,
                         activation='relu'),
                   Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((3,3,3)),
                   Conv3D(64,(3,3,3),activation='relu'),
                   Conv3D(64,(3,3,3),activation='relu'),
                   MaxPooling3D((3,3,3)),
                  Conv3D(128,(3,3,3),activation='relu'),
                    #Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((2,2,2)),

                   Flatten(),

                   Dense(512,activation='relu'),
                   Dropout(drprate),
                   Dense(256,activation='relu'),
                   Dropout(drprate),
                   Dense(128,activation='relu'),
                   Dropout(drprate),
                   Dense(64,activation='relu'),
                   Dense(1,activation='sigmoid')
  ])
  cnn.compile(optimizer=Adam(learning_rate=lr,clipnorm=1),metrics=['accuracy'],loss='binary_crossentropy')
  return cnn

cnn = create_cnn_sparse(0.0007,local_data.reshape(101,120,140,120,1)[0].shape)
custom_loop(cnn,120,90,True,0.6,0.7,'aug28normal_sparse_model')

cnn = create_cnn_sparse(0.0007,local_data.reshape(101,120,140,120,1)[0].shape)
custom_loop(cnn,120,70,True,0.6,0.6,'aug28normalsparserepeat')

cnn = create_cnn_sparse(0.0005,local_data.reshape(101,120,140,120,1)[0].shape)
custom_loop(cnn,120,70,True,0.6,0.6,'aug28normalsparserepeat_lower_lr')

# repeat 
cnn = create_cnn_sparse_drop(0.0007,local_data.reshape(101,120,140,120,1)[0].shape,0.2)
custom_loop(cnn,120,70,True,0.6,0.65,'tmp_increased_dropout',20)

# ! cp *h5 drive/My\ Drive/data/ 
cnn.summary()

import glob 
filelist = glob.glob('/content/*epoch*.h5')
filelist
for m in filelist:
  print(m)
  md = tf.keras.models.load_model(m)

  accu,_,yprob = eval_performance(md,local_scale.reshape(101,120,140,120,1),local_y)
  make_roc_curve(y_prob=yprob,acc=accu,truey=local_y,plot=True)
  plt.show()

! rm *h5

cnn = create_cnn_sparse_drop(0.0005,local_data.reshape(101,120,140,120,1)[0].shape,0.25)
custom_loop(cnn,120,70,True,0.6,0.65,'tmp_increased_dropout_lower_lr',24)

"""bookmark 3 

try to add some regularisation / bn on top of the current dropout
"""

def create_cnn_sparse_reg(lr,shape,drprate,reg):
  cnn = Sequential([
                  Conv3D(32,(3,3,3),data_format='channels_last',input_shape=shape,
                         activation='relu',kernel_regularizer=tf.keras.regularizers.l2(reg)),
                   Conv3D(32,(3,3,3),activation='relu',kernel_regularizer=tf.keras.regularizers.l2(reg)),
                    MaxPooling3D((3,3,3)),
                   Conv3D(64,(3,3,3),activation='relu',kernel_regularizer=tf.keras.regularizers.l2(reg)),
                   Conv3D(64,(3,3,3),activation='relu',kernel_regularizer=tf.keras.regularizers.l2(reg)),
                   MaxPooling3D((3,3,3)),
                  Conv3D(128,(3,3,3),activation='relu',kernel_regularizer=tf.keras.regularizers.l2(reg)),
                    #Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((2,2,2)),

                   Flatten(),

                   Dense(512,activation='relu'),
                   Dropout(drprate),
                   Dense(256,activation='relu'),
                   Dropout(drprate),
                   Dense(128,activation='relu'),
                   Dropout(drprate),
                   Dense(64,activation='relu'),
                   Dense(1,activation='sigmoid')
  ])
  cnn.compile(optimizer=Adam(learning_rate=lr,clipnorm=1),metrics=['accuracy'],loss='binary_crossentropy')
  return cnn



cnn = create_cnn_sparse_drop(0.0007,local_data.reshape(101,120,140,120,1)[0].shape,0.25)
custom_loop(cnn,160,100,True,0.6,0.65,'tmp_increased_drop',50)

"""bookmark 4. the models above aren't working that well. current best architecture is the original sparse arch with some dropout (that has seen vaying performance with different values). This is the point to go from tomorrow. There is also the issue of interpretation of the output of the sensitivity experiments...."""

def custom_loop_cross_val(model,epochs,burnin,plot,thresh,acc_thresh,npredictions,train_data,train_y,test_data,test_y):
  acc = []
  loss = []
  aucs = []
  accus = []
  for i in range(epochs):
    print(f'epoch {i+1} fit:')
    pr = model.fit(x=train_data,y=train_y,epochs=1,shuffle=True,batch_size=6,
                   verbose=1)
    acc.append(pr.history['accuracy'])
    loss.append(pr.history['loss'])
    acc_val = acc[-1][0]
    if acc_val > acc_thresh and i < burnin:
      print(f'Accuracy above threshold before burn in. New burn in assigned at {i}')
      burnin = i
    else:
      pass
    if i >= burnin:
      accu,_,yprob = eval_performance(model,test_data,test_y)
      aucs.append(make_roc_curve(y_prob=yprob,acc=accu,truey=test_y,plot=False))
      accus.append(accu)
     
    if i - burnin > npredictions:
      print(f'Model has been storing predictions for {npredictions} epochs. Terminating loop')
      break
    
  if plot==True:
    fig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(15,8))
    ax1.plot(acc,c='blue')
    ax1.set_title('Training accuracy')
    ax2.plot(loss,c='red')
    ax2.set_title('Training loss')

    accu,_,yprob = eval_performance(model,test_data,test_y)
    make_roc_curve(y_prob=yprob,acc=accu,truey=test_y,plot=True)
    

    fig,ax = plt.subplots(figsize=(15,8))
    ax.plot(aucs,c='green',label='AUCs')
    ax.set_title('AUCs vs epochs after burn in')
    ax.plot(accus,c='red',label='Test Accuracies')
    ax.legend()
    return([accus,aucs])
  return([accus,aucs])
#cnn = create_cnn_sparse(0.0007,local_data.reshape(101,120,140,120,1)[0].shape)

tf.keras.backend.clear_session()

import tensorflow as tf
print(tf.__version__)

# create and reshape total data t
total_scale = np.zeros((300,120,140,120,1))
total_scale[:101] = local_scale.reshape(101,120,140,120,1)
total_scale[101:] = ukb_scale.reshape(199,120,140,120,1)
total_y = local_y
for i in ukb_y:
  total_y.append(i)

from sklearn.model_selection import StratifiedKFold
import tensorflow as tf
def cross_val_cnn(folds,x,y):
    
    skf = StratifiedKFold(n_splits=folds,shuffle=True,random_state=42)
    accs = []
    aucs = []
    bp = y
    bp = np.array(bp)
    skf.get_n_splits(x,bp)
    count = 0
    for train_index,test_index in skf.split(x,bp):
        count += 1
        print(f'fold {count} commencing...')
        cnn = create_cnn_sparse_drop(0.0005,local_data.reshape(101,120,140,120,1)[0].shape,0.1)
        accus,aucs = custom_loop_cross_val(cnn,100,60,False,0.6,0.65,24,x[train_index],bp[train_index],x[test_index],bp[test_index])
        max_ind = np.argmax(accus)
        accs.append(accus[max_ind])
        aucs.append(aucs[max_ind])
        #preds = [nn.predict(np.array([x[i]])) for i in test_index]
        #preds = [1 if i > 0.5 else 0 for i in preds]
        #acc = accuracy_score(bp[test_index],preds,normalize=True)

        
    return([f'{folds} fold cross val (conv neural net): {np.mean(accs)} +/- {np.std(accs)} \n aucs: {np.mean(aucs)} + / - {np.std(aucs)}',
            accs,aucs])
msg,accs,aucs = cross_val_cnn(10,total_scale,total_y)
print(msg)

tf.keras.backend.clear_session()
#del cross_val_cnn
del tf
#np.save('accs_cnn.npy',np.array(accs))
#np.save('aucs_cnn.npy',np.array(aucs))

! cp *npy drive/My\ Drive/data/

! cp *h5 drive/My\ Drive/data/

a = 0
for i in range(20):
  
    
  a += 1
  if a > 10:
    break
  print(a)

mdlist[0].get_weights()[0][0][0][0] == mdlist[1].get_weights()[0][0][0][0]

fig,ax = plt.subplots()
ax.plot([2,4,2,4,2,4])
ax.set_xticklabels(np.arange(7)+30)

a = 6
b = 7

if a > 2 and b < 8:
  print('eys')

def create_cnn_sparse(lr,shape):
  cnn = Sequential([
                  Conv3D(16,(3,3,3),data_format='channels_last',input_shape=shape,
                         activation='relu'),
                   Conv3D(16,(3,3,3),activation='relu'),
                    MaxPooling3D((3,3,3)),
                   Conv3D(32,(3,3,3),activation='relu'),
                   Conv3D(32,(3,3,3),activation='relu'),
                   MaxPooling3D((3,3,3)),
                  Conv3D(32,(3,3,3),activation='relu'),
                    #Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((3,3,3)),

                   Flatten(),

                   Dense(512,activation='relu'),
                   Dropout(0.25),
                   Dense(256,activation='relu'),
                   Dropout(0.25),
                   Dense(128,activation='relu'),
                   Dropout(0.25),
                   Dense(64,activation='relu'),
                   Dense(1,activation='sigmoid')
  ])
  cnn.compile(optimizer=SGD(learning_rate=lr),metrics=['accuracy'],loss='binary_crossentropy')
  return cnn
cnn = create_cnn_sparse(0.0009,ukb_data[0].shape)
cnn.summary()
pr = cnn.fit(x=ukb_data,y=ukb_y,validation_data=(local_data,local_y),epochs=30,shuffle=True,batch_size=1)
plot_and_save_prog(pr,val=True)

def create_cnn_sparse(lr,shape):
  cnn = Sequential([
                  Conv3D(16,(3,3,3),data_format='channels_last',input_shape=shape,
                         activation='tanh'),
                   Conv3D(16,(3,3,3),activation='tanh'),
                    MaxPooling3D((3,3,3)),
                   Conv3D(32,(3,3,3),activation='tanh'),
                   Conv3D(32,(3,3,3),activation='tanh'),
                   MaxPooling3D((3,3,3)),
                  Conv3D(32,(3,3,3),activation='tanh'),
                    #Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((3,3,3)),

                   Flatten(),

                   Dense(512,activation='tanh'),
                   Dropout(0.1),
                   Dense(256,activation='tanh'),
                   Dropout(0.1),
                   Dense(128,activation='tanh'),
                   Dropout(0.1),
                   Dense(64,activation='tanh'),
                   Dense(1,activation='sigmoid')
  ])
  cnn.compile(optimizer=SGD(learning_rate=lr,clipnorm=1,nesterov=True,momentum=0.99),metrics=['accuracy'],loss='binary_crossentropy')
  return cnn
cnn = create_cnn_sparse(0.003,ukb_data[0].shape)
cnn.summary()
pr = cnn.fit(x=ukb_data,y=ukb_y,validation_data=(local_data,local_y),epochs=30,shuffle=True,batch_size=1)
plot_and_save_prog(pr,val=True)

# old data 
del data
data = np.load('drive/My Drive/data/cleaned_data.npy')
#data = np.load('drive/My Drive/data/local_and_ukb_dataset_lossy_revised_qc.npy')
ydata = np.load('drive/My Drive/data/labels.npy')

ukb_data=data[96:]
local_data=data[:96]
ukb_y=ydata[96:]
local_y=ydata[:96]

def datareshape(data):
  if len(data.shape) != 2:
    s = data.shape
    s = s + (1,)
    return data.reshape(s)
  else:
    return(data.reshape(data.shape[0],125,145,125,1))
ukb_data=datareshape(ukb_data)
local_data=datareshape(local_data)

def create_cnn_sparse(lr,shape):
  cnn = Sequential([
                  Conv3D(16,(3,3,3),data_format='channels_last',input_shape=shape,
                         activation='tanh'),
                   Conv3D(16,(3,3,3),activation='relu'),
                    MaxPooling3D((3,3,3)),
                   Conv3D(32,(3,3,3),activation='relu'),
                   Conv3D(32,(3,3,3),activation='relu'),
                   MaxPooling3D((3,3,3)),
                  Conv3D(32,(3,3,3),activation='relu'),
                    #Conv3D(32,(3,3,3),activation='relu'),
                    MaxPooling3D((3,3,3)),

                   Flatten(),

                   Dense(512,activation='relu'),
                   Dropout(0.1),
                   Dense(256,activation='relu'),
                   Dropout(0.1),
                   Dense(128,activation='relu'),
                   # Dropout(0.1),
                   Dense(64,activation='relu'),
                   Dense(1,activation='sigmoid')
  ])
  cnn.compile(optimizer=SGD(learning_rate=lr,clipnorm=1,nesterov=True,momentum=0.99),metrics=['accuracy'],loss='binary_crossentropy')
  return cnn
cnn = create_cnn_sparse(0.003,ukb_data[0].shape)
cnn.summary()
pr = cnn.fit(x=ukb_data,y=ukb_y,validation_data=(local_data,local_y),epochs=30,shuffle=True,batch_size=1)
plot_and_save_prog(pr,val=True)

# eval_performance here when this is finished